{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'WindowsPath' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 47>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     45\u001B[0m largeDir \u001B[38;5;241m=\u001B[39m pathlib\u001B[38;5;241m.\u001B[39mPath(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./Books\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     46\u001B[0m BookList \u001B[38;5;241m=\u001B[39m get_all_items(largeDir, SKIP)\n\u001B[1;32m---> 47\u001B[0m BookList \u001B[38;5;241m=\u001B[39m [item \u001B[38;5;28;01mfor\u001B[39;00m sublist \u001B[38;5;129;01min\u001B[39;00m BookList \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m sublist]\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# clean the dataset\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# for path in BookList:\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m#     print(path)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m#         os.remove(path)\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;66;03m#     continue\u001B[39;00m\n\u001B[0;32m     61\u001B[0m bigString \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     45\u001B[0m largeDir \u001B[38;5;241m=\u001B[39m pathlib\u001B[38;5;241m.\u001B[39mPath(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./Books\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     46\u001B[0m BookList \u001B[38;5;241m=\u001B[39m get_all_items(largeDir, SKIP)\n\u001B[1;32m---> 47\u001B[0m BookList \u001B[38;5;241m=\u001B[39m [item \u001B[38;5;28;01mfor\u001B[39;00m sublist \u001B[38;5;129;01min\u001B[39;00m BookList \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m sublist]\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# clean the dataset\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# for path in BookList:\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m#     print(path)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m#         os.remove(path)\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;66;03m#     continue\u001B[39;00m\n\u001B[0;32m     61\u001B[0m bigString \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'WindowsPath' object is not iterable"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "from keras.utils import pad_sequences\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# simplified chinese tokenizer\n",
    "import jieba\n",
    "import time\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.《》（）+-=()\"\"''/=\"\n",
    "\n",
    "# skipped directories\n",
    "# SKIP = [\"0\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "SKIP = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "# SKIP = []\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "def get_all_items(root: pathlib.Path, exclude):\n",
    "    itemList = []\n",
    "    for item in root.iterdir():\n",
    "        if item.name in exclude:\n",
    "            continue\n",
    "        if item.is_dir():\n",
    "            itemList.append(get_all_items(item, []))\n",
    "            continue\n",
    "        itemList.append(item)\n",
    "    return itemList\n",
    "\n",
    "\n",
    "# begin preprocessing\n",
    "largeDir = pathlib.Path(\"./Books\")\n",
    "BookList = get_all_items(largeDir, SKIP)\n",
    "BookList = [item for sublist in BookList for item in sublist]\n",
    "\n",
    "\n",
    "# clean the dataset\n",
    "# for path in BookList:\n",
    "#     print(path)\n",
    "#     file = open(path, 'r')\n",
    "#     try:\n",
    "#         fileStr = file.read()\n",
    "#     except UnicodeDecodeError as error:\n",
    "#         file.close()\n",
    "#         os.remove(path)\n",
    "#     continue\n",
    "\n",
    "bigString = \"\"\n",
    "\n",
    "for path in BookList:\n",
    "    with open(path, 'r') as fiction:\n",
    "        bigString += fiction.read()\n",
    "\n",
    "# methods to strip punctuation and symbols\n",
    "# bigString = re.sub(r\"[%s]+\" %punc, \"\", bigString)\n",
    "bigString = re.sub(r'[^\\w\\s]', '', bigString)\n",
    "\n",
    "# list of the words in their original order\n",
    "allTokens = jieba.lcut(bigString, cut_all=False)\n",
    "t2 = time.time()\n",
    "print(\"Runtime for this cell in seconds: \", t2 - t1)\n",
    "print(\"Corpus length in words: \", len(allTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "\n",
    "os.remove('vocab.txt')\n",
    "minFreq = 10\n",
    "wordFreq = {}\n",
    "for token in allTokens:\n",
    "    wordFreq[token] = wordFreq.get(token, 0) + 1\n",
    "\n",
    "rareWords = set()\n",
    "for k, v in wordFreq.items():\n",
    "    if wordFreq[k] < minFreq:\n",
    "        rareWords.add(k)\n",
    "\n",
    "words = set(allTokens)\n",
    "print(\"Unique words before filter: \", len(words))\n",
    "print(\"To reduce vocab size, neglect words with appearances < \", minFreq)\n",
    "words = sorted(set(words) - rareWords)\n",
    "print(\"Unique words after filter: \", len(words))\n",
    "\n",
    "\n",
    "words_file_path = \"vocab.txt\"\n",
    "\n",
    "words_file = open(words_file_path, 'w')\n",
    "for w in words:\n",
    "    print(w)\n",
    "    if w != \"\\n\":\n",
    "        words_file.write(w)\n",
    "        words_file.write(\"\\n\")\n",
    "    else:\n",
    "        words_file.write(w)\n",
    "words_file.close()\n",
    "\n",
    "wordAsKey = dict((c, i) for i, c in enumerate(words))\n",
    "intAsKey = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "seqLen = 50\n",
    "step = 1\n",
    "sequences = []\n",
    "nextWords = []\n",
    "seqIgnored = 0\n",
    "for i in range(0, len(allTokens) - seqLen, step):\n",
    "    if len(set(allTokens[i:i+seqLen+1]).intersection(rareWords)) == 0:\n",
    "        sequences.append(allTokens[i:i + seqLen])\n",
    "        nextWords.append(allTokens[i + seqLen])\n",
    "    else:\n",
    "        seqIgnored += 1\n",
    "\n",
    "print(\"Number of sequences ignored: \", seqIgnored)\n",
    "print(\"Number of remaining sequences: \", len(sequences))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Runtime for this cell in seconds: \", t2 - t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "(sentences, nextWords), (testSentences, testNextWords) = shuffle_and_split_training_set(sequences, nextWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "def get_model(dropout=0.2):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128), input_shape=(seqLen, len(words))))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "import os, psutil\n",
    "process = psutil.Process()\n",
    "print(process.memory_info().rss/1024/1024)\n",
    "\n",
    "checkPath = \"./LSTM_Fic-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "                \"loss{loss:.4f}-accuracy{accuracy:.4f}-val_loss{val_loss:.4f}-val_accuracy{val_accuracy:.4f}\" % \\\n",
    "                (len(words), seqLen, minFreq)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "examples = \"examples.txt\"\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+testSentences))\n",
    "    seed = (sentences+testSentences)[seed_index]\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, seqLen, len(words)))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t, wordAsKey[word]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = intAsKey[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "            examples_file.write(\" \"+next_word)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "    examples_file.flush()\n",
    "\n",
    "\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, seqLen, len(words)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t, wordAsKey[w]] = 1\n",
    "            y[i, wordAsKey[next_word_list[index % len(sentence_list)]]] = 1\n",
    "            index = index + 1\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkPath, monitor='val_accuracy', save_best_only=True)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "examples_file = open(examples, \"a\")\n",
    "model.fit_generator(generator(sentences, nextWords, BATCH_SIZE),\n",
    "                        steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                        epochs=100,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=generator(testSentences, testNextWords, BATCH_SIZE),\n",
    "                        validation_steps=int(len(testSentences)/BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# below code is too slow\n",
    "# freqDist = nltk.FreqDist(allTokens)\n",
    "# words = freqDist.most_common(50000)\n",
    "#\n",
    "# afterRareTokens = [word for word in allTokens if word in words]\n",
    "#\n",
    "# charSet = set(afterRareTokens)\n",
    "# chars = sorted(list(charSet))\n",
    "# char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "#\n",
    "# numWords = len(afterRareTokens)\n",
    "# numVocab = len(charSet)\n",
    "#\n",
    "# print(afterRareTokens)\n",
    "#\n",
    "# print(\"Number of words: \", numWords)\n",
    "# print(\"Vocab size: \", numVocab)\n",
    "\n",
    "# seqLen = 100\n",
    "# dataX = []\n",
    "# dataY = []\n",
    "# for i in range(0, numWords - seqLen, 1):\n",
    "#     seqIn = allTokens[i:i + seqLen]\n",
    "#     seqOut = allTokens[i + seqLen]\n",
    "#     dataX.append([char_to_int[char] for char in seqIn])\n",
    "#     dataY.append(char_to_int[seqOut])\n",
    "# nPatterns = len(dataX)\n",
    "# print(\"Total Patterns: \", nPatterns)\n",
    "#\n",
    "# # reshape X to be [samples, time steps, features]\n",
    "# X = np.reshape(dataX, (nPatterns, seqLen, 1))\n",
    "# # normalize\n",
    "# X = X / float(numVocab)\n",
    "# # one hot encode the output variable\n",
    "# y = to_categorical(dataY)\n",
    "# print(y)\n",
    "# print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}