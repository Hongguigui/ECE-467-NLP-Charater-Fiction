{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime for this cell in seconds:  41.138999938964844\n",
      "Corpus length in words:  6407510\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "from keras.utils import pad_sequences\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# simplified chinese tokenizer\n",
    "import jieba\n",
    "import time\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.《》（）+-=()\"\"''/=\"\n",
    "\n",
    "# skipped directories\n",
    "# SKIP = [\"0\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "SKIP = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "# SKIP = []\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "def get_all_items(root: pathlib.Path, exclude):\n",
    "    itemList = []\n",
    "    for item in root.iterdir():\n",
    "        if item.name in exclude:\n",
    "            continue\n",
    "        if item.is_dir():\n",
    "            itemList.append(get_all_items(item, []))\n",
    "            continue\n",
    "        itemList.append(item)\n",
    "    return itemList\n",
    "\n",
    "\n",
    "# begin preprocessing\n",
    "largeDir = pathlib.Path(\"./Books\")\n",
    "BookList = get_all_items(largeDir, SKIP)\n",
    "BookList = [item for sublist in BookList for item in sublist]\n",
    "\n",
    "\n",
    "# clean the dataset\n",
    "# for path in BookList:\n",
    "#     print(path)\n",
    "#     file = open(path, 'r')\n",
    "#     try:\n",
    "#         fileStr = file.read()\n",
    "#     except UnicodeDecodeError as error:\n",
    "#         file.close()\n",
    "#         os.remove(path)\n",
    "#     continue\n",
    "\n",
    "bigString = \"\"\n",
    "\n",
    "for path in BookList:\n",
    "    with open(path, 'r') as fiction:\n",
    "        bigString += fiction.read()\n",
    "\n",
    "# methods to strip punctuation and symbols\n",
    "# bigString = re.sub(r\"[%s]+\" %punc, \"\", bigString)\n",
    "bigString = re.sub(r'[^\\w\\s]', '', bigString)\n",
    "\n",
    "# list of the words in their original order\n",
    "allTokens = jieba.lcut(bigString, cut_all=False)\n",
    "t2 = time.time()\n",
    "print(\"Runtime for this cell in seconds: \", t2 - t1)\n",
    "print(\"Corpus length in words: \", len(allTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before filter:  166485\n",
      "To reduce vocab size, neglect words with appearances <  5\n",
      "Unique words after filter:  52982\n"
     ]
    }
   ],
   "source": [
    "minFreq = 5\n",
    "wordFreq = {}\n",
    "for token in allTokens:\n",
    "    wordFreq[token] = wordFreq.get(token, 0) + 1\n",
    "\n",
    "rareWords = set()\n",
    "for k, v in wordFreq.items():\n",
    "    if wordFreq[k] < minFreq:\n",
    "        rareWords.add(k)\n",
    "\n",
    "words = set(allTokens)\n",
    "print(\"Unique words before filter: \", len(words))\n",
    "print(\"To reduce vocab size, neglect words with appearances < \", minFreq)\n",
    "words = sorted(set(words) - rareWords)\n",
    "print(\"Unique words after filter: \", len(words))\n",
    "\n",
    "word_to_int = dict((c, i) for i, c in enumerate(words))\n",
    "int_to_word = dict((i, c) for i, c in enumerate(words))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences ignored:  4443219\n",
      "Number of remaining sequences:  1964241\n",
      "Runtime for this cell in seconds:  19.494029760360718\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "seqLen = 50\n",
    "step = 1\n",
    "sequences = []\n",
    "nextWords = []\n",
    "seqIgnored = 0\n",
    "for i in range(0, len(allTokens) - seqLen, step):\n",
    "    if len(set(allTokens[i:i+seqLen+1]).intersection(rareWords)) == 0:\n",
    "        sequences.append(allTokens[i:i + seqLen])\n",
    "        nextWords.append(allTokens[i + seqLen])\n",
    "    else:\n",
    "        seqIgnored += 1\n",
    "\n",
    "print(\"Number of sequences ignored: \", seqIgnored)\n",
    "print(\"Number of remaining sequences: \", len(sequences))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Runtime for this cell in seconds: \", t2 - t1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 1924956\n",
      "Size of test set = 39285\n"
     ]
    }
   ],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "(sentences, nextWords), (testSentences, testNextWords) = shuffle_and_split_training_set(sequences, nextWords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "def get_model(dropout=0.2):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128), input_shape=(seqLen, len(words))))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkPath = \"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# below code is too slow\n",
    "# freqDist = nltk.FreqDist(allTokens)\n",
    "# words = freqDist.most_common(50000)\n",
    "#\n",
    "# afterRareTokens = [word for word in allTokens if word in words]\n",
    "#\n",
    "# charSet = set(afterRareTokens)\n",
    "# chars = sorted(list(charSet))\n",
    "# char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "#\n",
    "# numWords = len(afterRareTokens)\n",
    "# numVocab = len(charSet)\n",
    "#\n",
    "# print(afterRareTokens)\n",
    "#\n",
    "# print(\"Number of words: \", numWords)\n",
    "# print(\"Vocab size: \", numVocab)\n",
    "\n",
    "# seqLen = 100\n",
    "# dataX = []\n",
    "# dataY = []\n",
    "# for i in range(0, numWords - seqLen, 1):\n",
    "#     seqIn = allTokens[i:i + seqLen]\n",
    "#     seqOut = allTokens[i + seqLen]\n",
    "#     dataX.append([char_to_int[char] for char in seqIn])\n",
    "#     dataY.append(char_to_int[seqOut])\n",
    "# nPatterns = len(dataX)\n",
    "# print(\"Total Patterns: \", nPatterns)\n",
    "#\n",
    "# # reshape X to be [samples, time steps, features]\n",
    "# X = np.reshape(dataX, (nPatterns, seqLen, 1))\n",
    "# # normalize\n",
    "# X = X / float(numVocab)\n",
    "# # one hot encode the output variable\n",
    "# y = to_categorical(dataY)\n",
    "# print(y)\n",
    "# print(y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}