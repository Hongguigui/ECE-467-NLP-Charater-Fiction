{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\PC\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.661 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 81>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     78\u001B[0m bigString \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[^\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, bigString)\n\u001B[0;32m     80\u001B[0m \u001B[38;5;66;03m# list of the words in their original order\u001B[39;00m\n\u001B[1;32m---> 81\u001B[0m allTokens \u001B[38;5;241m=\u001B[39m \u001B[43mjieba\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlcut\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbigString\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcut_all\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCorpus length in words: \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(allTokens))\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# os.remove('vocab.txt')\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\jieba\\__init__.py:357\u001B[0m, in \u001B[0;36mTokenizer.lcut\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    356\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlcut\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcut\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\jieba\\__init__.py:325\u001B[0m, in \u001B[0;36mTokenizer.cut\u001B[1;34m(self, sentence, cut_all, HMM, use_paddle)\u001B[0m\n\u001B[0;32m    323\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m re_han\u001B[38;5;241m.\u001B[39mmatch(blk):\n\u001B[1;32m--> 325\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m cut_block(blk):\n\u001B[0;32m    326\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m word\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\jieba\\__init__.py:269\u001B[0m, in \u001B[0;36mTokenizer.__cut_DAG\u001B[1;34m(self, sentence)\u001B[0m\n\u001B[0;32m    267\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mFREQ\u001B[38;5;241m.\u001B[39mget(buf):\n\u001B[0;32m    268\u001B[0m     recognized \u001B[38;5;241m=\u001B[39m finalseg\u001B[38;5;241m.\u001B[39mcut(buf)\n\u001B[1;32m--> 269\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m recognized:\n\u001B[0;32m    270\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m t\n\u001B[0;32m    271\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\jieba\\finalseg\\__init__.py:90\u001B[0m, in \u001B[0;36mcut\u001B[1;34m(sentence)\u001B[0m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m blocks:\n\u001B[0;32m     89\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m re_han\u001B[38;5;241m.\u001B[39mmatch(blk):\n\u001B[1;32m---> 90\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m __cut(blk):\n\u001B[0;32m     91\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m Force_Split_Words:\n\u001B[0;32m     92\u001B[0m                 \u001B[38;5;28;01myield\u001B[39;00m word\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\jieba\\finalseg\\__init__.py:61\u001B[0m, in \u001B[0;36m__cut\u001B[1;34m(sentence)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__cut\u001B[39m(sentence):\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mglobal\u001B[39;00m emit_P\n\u001B[1;32m---> 61\u001B[0m     prob, pos_list \u001B[38;5;241m=\u001B[39m \u001B[43mviterbi\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBMES\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_P\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans_P\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43memit_P\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m     begin, nexti \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;66;03m# print pos_list, sentence\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\jieba\\finalseg\\__init__.py:49\u001B[0m, in \u001B[0;36mviterbi\u001B[1;34m(obs, states, start_p, trans_p, emit_p)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m states:\n\u001B[0;32m     47\u001B[0m     em_p \u001B[38;5;241m=\u001B[39m emit_p[y]\u001B[38;5;241m.\u001B[39mget(obs[t], MIN_FLOAT)\n\u001B[0;32m     48\u001B[0m     (prob, state) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\n\u001B[1;32m---> 49\u001B[0m         [(V[t \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m][y0] \u001B[38;5;241m+\u001B[39m trans_p[y0]\u001B[38;5;241m.\u001B[39mget(y, MIN_FLOAT) \u001B[38;5;241m+\u001B[39m em_p, y0) \u001B[38;5;28;01mfor\u001B[39;00m y0 \u001B[38;5;129;01min\u001B[39;00m PrevStatus[y]])\n\u001B[0;32m     50\u001B[0m     V[t][y] \u001B[38;5;241m=\u001B[39m prob\n\u001B[0;32m     51\u001B[0m     newpath[y] \u001B[38;5;241m=\u001B[39m path[state] \u001B[38;5;241m+\u001B[39m [y]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\jieba\\finalseg\\__init__.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m states:\n\u001B[0;32m     47\u001B[0m     em_p \u001B[38;5;241m=\u001B[39m emit_p[y]\u001B[38;5;241m.\u001B[39mget(obs[t], MIN_FLOAT)\n\u001B[0;32m     48\u001B[0m     (prob, state) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\n\u001B[1;32m---> 49\u001B[0m         [(V[t \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m][y0] \u001B[38;5;241m+\u001B[39m trans_p[y0]\u001B[38;5;241m.\u001B[39mget(y, MIN_FLOAT) \u001B[38;5;241m+\u001B[39m em_p, y0) \u001B[38;5;28;01mfor\u001B[39;00m y0 \u001B[38;5;129;01min\u001B[39;00m PrevStatus[y]])\n\u001B[0;32m     50\u001B[0m     V[t][y] \u001B[38;5;241m=\u001B[39m prob\n\u001B[0;32m     51\u001B[0m     newpath[y] \u001B[38;5;241m=\u001B[39m path[state] \u001B[38;5;241m+\u001B[39m [y]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "from keras.utils import pad_sequences\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# simplified chinese tokenizer\n",
    "import jieba\n",
    "import time\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.《》（）+-=()\"\"''/=\"\n",
    "\n",
    "# skipped directories\n",
    "# SKIP = [\"0\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "SKIP = [\"0\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\",\n",
    "        \"w\", \"x\", \"y\", \"z\"]\n",
    "# SKIP = []\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "def get_all_items(root: pathlib.Path, exclude):\n",
    "    itemList = []\n",
    "    for item in root.iterdir():\n",
    "        if item.name in exclude:\n",
    "            continue\n",
    "        if item.is_dir():\n",
    "            itemList.append(get_all_items(item, []))\n",
    "            continue\n",
    "        itemList.append(item)\n",
    "    return itemList\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# begin preprocessing\n",
    "largeDir = pathlib.Path(\"./Books\")\n",
    "BookList = get_all_items(largeDir, SKIP)\n",
    "BookList = [item for sublist in BookList for item in sublist]\n",
    "\n",
    "# clean the dataset\n",
    "# for path in BookList:\n",
    "#     print(path)\n",
    "#     file = open(path, 'r')\n",
    "#     try:\n",
    "#         fileStr = file.read()\n",
    "#     except UnicodeDecodeError as error:\n",
    "#         file.close()\n",
    "#         os.remove(path)\n",
    "#     continue\n",
    "\n",
    "bigString = \"\"\n",
    "\n",
    "for path in BookList:\n",
    "    with open(path, 'r') as fiction:\n",
    "        bigString += fiction.read()\n",
    "\n",
    "# methods to strip punctuation and symbols\n",
    "# bigString = re.sub(r\"[%s]+\" %punc, \"\", bigString)\n",
    "bigString = re.sub(r'[^\\w\\s]', '', bigString)\n",
    "\n",
    "# list of the words in their original order\n",
    "allTokens = jieba.lcut(bigString, cut_all=False)\n",
    "\n",
    "\n",
    "print(\"Corpus length in words: \", len(allTokens))\n",
    "\n",
    "# os.remove('vocab.txt')\n",
    "\n",
    "minFreq = 300\n",
    "maxFreq = 5000000\n",
    "wordFreq = {}\n",
    "for token in allTokens:\n",
    "    wordFreq[token] = wordFreq.get(token, 0) + 1\n",
    "\n",
    "skipWords = set()\n",
    "for k, v in wordFreq.items():\n",
    "    if wordFreq[k] < minFreq or wordFreq[k] > maxFreq:\n",
    "        skipWords.add(k)\n",
    "    elif k.isascii():\n",
    "        skipWords.add(k)\n",
    "\n",
    "skipWords.remove(\" \")\n",
    "words = set(allTokens)\n",
    "print(\"Unique words before filter: \", len(words))\n",
    "print(\"To reduce vocab size, neglect words with appearances < \", minFreq)\n",
    "words = sorted(set(words) - skipWords)\n",
    "print(\"Unique words after filter: \", len(words))\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Runtime for this cell in seconds: \", t2 - t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "words_file_path = \"vocab.txt\"\n",
    "\n",
    "words_file = codecs.open(words_file_path, 'w', encoding='gbk')\n",
    "# hugeStr = \"\".join(str(words))\n",
    "# words_file.write(hugeStr)\n",
    "\n",
    "for w in words:\n",
    "    if w != \"\\n\":\n",
    "        words_file.write(w)\n",
    "        words_file.write(\"\\n\")\n",
    "    else:\n",
    "        words_file.write(str(w))\n",
    "words_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}