{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "import os\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "import numpy as np\n",
    "import jieba\n",
    "import time\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.《》（）+-=()\"\"''/=\"\n",
    "\n",
    "# skipped directories\n",
    "# SKIP = [\"0\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "SKIP = [\"0\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "# SKIP = []\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "def get_all_items(root: pathlib.Path, exclude):\n",
    "    itemList = []\n",
    "    for item in root.iterdir():\n",
    "        if item.name in exclude:\n",
    "            continue\n",
    "        if item.is_dir():\n",
    "            itemList.append(get_all_items(item, []))\n",
    "            continue\n",
    "        itemList.append(item)\n",
    "    return itemList\n",
    "\n",
    "\n",
    "# begin preprocessing\n",
    "largeDir = pathlib.Path(\"./Books\")\n",
    "# largeDir = pathlib.Path(\"./Books\")\n",
    "BookList = get_all_items(largeDir, SKIP)\n",
    "BookList = [item for sublist in BookList for item in sublist]\n",
    "\n",
    "\n",
    "# clean the dataset\n",
    "# for path in BookList:\n",
    "#     print(path)\n",
    "#     file = open(path, 'r')\n",
    "#     try:\n",
    "#         fileStr = file.read()\n",
    "#     except UnicodeDecodeError as error:\n",
    "#         file.close()\n",
    "#         os.remove(path)\n",
    "#     continue\n",
    "\n",
    "bigString = \"\"\n",
    "\n",
    "for path in BookList:\n",
    "    with open(path, 'r', encoding='gbk') as fiction:\n",
    "        bigString += fiction.read()\n",
    "\n",
    "# methods to strip punctuation and symbols\n",
    "# bigString = re.sub(r\"[%s]+\" %punc, \"\", bigString)\n",
    "bigString = re.sub(r'[^\\w\\s]', '', bigString)\n",
    "\n",
    "# list of the words in their original order\n",
    "allTokens = jieba.lcut(bigString, cut_all=False)\n",
    "t2 = time.time()\n",
    "print(\"Runtime for this cell in seconds: \", t2 - t1)\n",
    "print(\"Corpus length in words: \", len(allTokens))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nFNI0rvYwWXu",
    "outputId": "264c9567-cbe9-4692-b62b-2696e4c56c3c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682558346696,
     "user_tz": 240,
     "elapsed": 62127,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:04:53.414871Z",
     "iopub.execute_input": "2023-04-30T18:04:53.415490Z",
     "iopub.status.idle": "2023-04-30T18:05:38.262779Z",
     "shell.execute_reply.started": "2023-04-30T18:04:53.415457Z",
     "shell.execute_reply": "2023-04-30T18:05:38.260212Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\PC\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.657 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime for this cell in seconds:  104.71058130264282\n",
      "Corpus length in words:  14869170\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "minFreq = 1000\n",
    "maxFreq = 50000000\n",
    "wordFreq = {}\n",
    "for token in allTokens:\n",
    "    wordFreq[token] = wordFreq.get(token, 0) + 1\n",
    "\n",
    "skipWords = set()\n",
    "for k, v in wordFreq.items():\n",
    "    if wordFreq[k] < minFreq or wordFreq[k] > maxFreq:    \n",
    "        skipWords.add(k)\n",
    "    elif k.isascii():\n",
    "        skipWords.add(k)\n",
    "\n",
    "# skipWords.remove(\"\\n\")\n",
    "skipWords.remove(\" \")\n",
    "words = set(allTokens)\n",
    "print(\"Unique words before filter: \", len(words))\n",
    "print(\"To reduce vocab size, neglect words with appearances < \", minFreq)\n",
    "print(\"To reduce vocab size, neglect words with appearances > \", maxFreq)\n",
    "words = sorted(set(words) - skipWords)\n",
    "print(\"Unique words after filter: \", len(words))\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yAEhwscIwWXy",
    "outputId": "e0da0173-33be-40c7-d3a5-0c12be1c3b13",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682558348613,
     "user_tz": 240,
     "elapsed": 1941,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:05:38.263927Z",
     "iopub.status.idle": "2023-04-30T18:05:38.265005Z",
     "shell.execute_reply.started": "2023-04-30T18:05:38.264680Z",
     "shell.execute_reply": "2023-04-30T18:05:38.264725Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before filter:  194511\n",
      "To reduce vocab size, neglect words with appearances <  1000\n",
      "To reduce vocab size, neglect words with appearances >  50000000\n",
      "Unique words after filter:  1490\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "words_file_path = \"vocab.txt\"\n",
    "\n",
    "words_file = codecs.open(words_file_path, 'w', encoding='gbk')\n",
    "# hugeStr = \"\".join(str(words))\n",
    "# words_file.write(hugeStr)\n",
    "\n",
    "for w in words:\n",
    "    if w != \"\\n\":\n",
    "        words_file.write(w)\n",
    "        words_file.write(\"\\n\")\n",
    "    else:\n",
    "        words_file.write(str(w))\n",
    "words_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# vocabFile = \"vocab.txt\"\n",
    "#\n",
    "# with open(vocabFile, 'r') as vocabulary:\n",
    "#     vocab = []\n",
    "#     for line in vocabulary:\n",
    "#         tmp_line = line.rstrip(\"\\n\")\n",
    "#         vocab.append(tmp_line)\n",
    "#\n",
    "# word_indices = dict((c, i) for i, c in enumerate(vocab))\n",
    "# indices_word = dict((i, c) for i, c in enumerate(vocab))\n",
    "# print(word_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# print(word_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "t1 = time.time()\n",
    "seqLen = 10\n",
    "step = 1\n",
    "sequences = []\n",
    "nextWords = []\n",
    "seqIgnored = 0\n",
    "for i in range(0, len(allTokens) - seqLen, step):\n",
    "    if len(set(allTokens[i:i+seqLen+1]).intersection(skipWords)) == 0:\n",
    "        sequences.append(allTokens[i:i + seqLen])\n",
    "        nextWords.append(allTokens[i + seqLen])\n",
    "    else:\n",
    "        seqIgnored += 1\n",
    "\n",
    "print(\"Number of sequences ignored: \", seqIgnored)\n",
    "print(\"Number of remaining sequences: \", len(sequences))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Runtime for this cell in seconds: \", t2 - t1)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "pggs1zNAwWXz",
    "outputId": "98cd9378-6f1a-4dc4-bddb-f4cac02c6b2a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682558405241,
     "user_tz": 240,
     "elapsed": 56630,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:05:38.267558Z",
     "iopub.status.idle": "2023-04-30T18:05:38.268117Z",
     "shell.execute_reply.started": "2023-04-30T18:05:38.267825Z",
     "shell.execute_reply": "2023-04-30T18:05:38.267853Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences ignored:  14502002\n",
      "Number of remaining sequences:  367158\n",
      "Runtime for this cell in seconds:  20.868000268936157\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=10):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "(sentences, nextWordsTrain), (testSentences, testNextWords) = shuffle_and_split_training_set(sequences, nextWords)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dABfcglLwWX0",
    "outputId": "a8468807-9fa2-4e39-a86f-764050b3750e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682558407689,
     "user_tz": 240,
     "elapsed": 2481,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:05:38.270483Z",
     "iopub.status.idle": "2023-04-30T18:05:38.271082Z",
     "shell.execute_reply.started": "2023-04-30T18:05:38.270767Z",
     "shell.execute_reply": "2023-04-30T18:05:38.270794Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 330442\n",
      "Size of test set = 36716\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['你', '是', '谁', '难道', '你', '就是', '传说', '中', '最', '强大'], ['只是', '自己', '刚才', '既然', '已经', '承认', '了', '这样', '的', '关系'], ['一样', '会', '发生', '今天', '这事', '而', '父亲', '却', '让', '她'], ['\\u3000', '好', '吧', '这话', '也', '算是', '提醒', '宋盈菲', '她', '处'], ['见', '张舒婷', '仍然', '在', '车', '里面', '坐', '着', '石林', '立即'], ['每', '过', '一天', '对于', '他', '来说', '就', '像', '过', '了'], ['这', '两人', '他', '绝对', '会', '让', '对方', '消失', '在', '这个'], ['不', '重要', '了', '我', '很', '高兴', '你', '可以', '活着', '这样'], ['去', '了', '之后', '问题', '越来越', '多', '解决', '一件', '就', '又'], ['了', '自己', '是', '在', '比赛', '之中', '似乎', '又', '回到', '了']]\n",
      "['的', '也', '和', '的', '明白', '一年', '世界', '就', '来', '那些']\n"
     ]
    }
   ],
   "source": [
    "print(testSentences[:10])\n",
    "print(testNextWords[:10])\n",
    "\n",
    "sentences_train = \"sentences_train.txt\"\n",
    "nextWords_train = \"nextWords_train.txt\"\n",
    "sentences_tests = \"sentences_tests.txt\"\n",
    "nextWords_tests = \"nextWords_tests.txt\"\n",
    "\n",
    "with open(sentences_train, \"w\") as sentences_file:\n",
    "    for sentence in sentences:\n",
    "        tmp_sentence = \",\".join(sentence)\n",
    "        sentences_file.write(f\"{tmp_sentence}\\n\")\n",
    "\n",
    "with open(sentences_tests, \"w\") as sentences_tests_file:\n",
    "    for sentence in testSentences:\n",
    "        tmp_sentence = \",\".join(sentence)\n",
    "        sentences_tests_file.write(f\"{tmp_sentence}\\n\")\n",
    "\n",
    "with open(nextWords_train, \"w\") as nextWords_testFile:\n",
    "    for nextword in nextWordsTrain:\n",
    "        nextWords_testFile.write(f\"{nextword}\\n\")\n",
    "\n",
    "with open(nextWords_tests, \"w\") as nextWords_testFile:\n",
    "    for nextword in testNextWords:\n",
    "        nextWords_testFile.write(f\"{nextword}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['你', '是', '谁', '难道', '你', '就是', '传说', '中', '最', '强大'], ['只是', '自己', '刚才', '既然', '已经', '承认', '了', '这样', '的', '关系'], ['一样', '会', '发生', '今天', '这事', '而', '父亲', '却', '让', '她'], ['\\u3000', '好', '吧', '这话', '也', '算是', '提醒', '宋盈菲', '她', '处'], ['见', '张舒婷', '仍然', '在', '车', '里面', '坐', '着', '石林', '立即'], ['每', '过', '一天', '对于', '他', '来说', '就', '像', '过', '了'], ['这', '两人', '他', '绝对', '会', '让', '对方', '消失', '在', '这个'], ['不', '重要', '了', '我', '很', '高兴', '你', '可以', '活着', '这样'], ['去', '了', '之后', '问题', '越来越', '多', '解决', '一件', '就', '又'], ['了', '自己', '是', '在', '比赛', '之中', '似乎', '又', '回到', '了']]\n",
      "['的', '也', '和', '的', '明白', '一年', '世界', '就', '来', '那些']\n"
     ]
    }
   ],
   "source": [
    "with open(sentences_train, \"r\") as sentences_file:\n",
    "    lines = []\n",
    "    for line in sentences_file:\n",
    "        tmp_line = line.rstrip(\"\\n\").split(\",\")\n",
    "        lines.append(tmp_line)\n",
    "\n",
    "with open(nextWords_train, \"r\") as answers_file:\n",
    "    answers = []\n",
    "    for line in answers_file:\n",
    "        tmp_line = line.rstrip(\"\\n\")\n",
    "        answers.append(tmp_line)\n",
    "\n",
    "with open(sentences_tests, \"r\") as sentences_tests_file:\n",
    "    lines_tests = []\n",
    "    for line in sentences_tests_file:\n",
    "        tmp_line = line.rstrip(\"\\n\").split(\",\")\n",
    "        lines_tests.append(tmp_line)\n",
    "\n",
    "with open(nextWords_tests, \"r\") as answers_tests_file:\n",
    "    answers_tests = []\n",
    "    for line in answers_tests_file:\n",
    "        tmp_line = line.rstrip(\"\\n\")\n",
    "        answers_tests.append(tmp_line)\n",
    "\n",
    "\n",
    "print(lines_tests[:10])\n",
    "print(answers_tests[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['的', '疑惑', '我', '可以', '慢慢', '给', '你', '解释', '你', '将']\n",
      "['的', '疑惑', '我', '可以', '慢慢', '给', '你', '解释', '你', '将']\n"
     ]
    }
   ],
   "source": [
    "print(lines[14948])\n",
    "print(sentences[14948])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# q = 0\n",
    "# trainDir = \"./Books/Train/\"\n",
    "# for filename in os.listdir(trainDir):\n",
    "#     fn = trainDir + str(q) + \".txt\"\n",
    "#     originalName = trainDir + filename\n",
    "#     os.rename(originalName,fn)\n",
    "#     q += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# model\n",
    "def get_model(dropout=0.2):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128), input_shape=(seqLen, len(words))))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "QdOxPqChwWX1",
    "outputId": "4224a27f-fe66-4800-b06d-686694993531",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682558408650,
     "user_tz": 240,
     "elapsed": 965,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:05:38.273654Z",
     "iopub.status.idle": "2023-04-30T18:05:38.274257Z",
     "shell.execute_reply.started": "2023-04-30T18:05:38.273958Z",
     "shell.execute_reply": "2023-04-30T18:05:38.273986Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 256)              1657856   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1490)              382930    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1490)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,040,786\n",
      "Trainable params: 2,040,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "# from keras.models import load_model\n",
    "# import os, psutil\n",
    "#\n",
    "#\n",
    "# seed(1)\n",
    "# tf.random.set_seed(2)\n",
    "#\n",
    "# process = psutil.Process()\n",
    "# print(process.memory_info().rss/1024/1024)\n",
    "# print(psutil.virtual_memory())\n",
    "#\n",
    "#\n",
    "# def sample(preds, temperature=1.0):\n",
    "#     # helper function to sample an index from a probability array\n",
    "#     preds = np.asarray(preds).astype('float64')\n",
    "#     preds = np.log(preds) / temperature\n",
    "#     exp_preds = np.exp(preds)\n",
    "#     preds = exp_preds / np.sum(exp_preds)\n",
    "#     probas = np.random.multinomial(1, preds, 1)\n",
    "#     return np.argmax(probas)\n",
    "#\n",
    "# examples = \"/kaggle/working/examples.txt\"\n",
    "#\n",
    "# # os.remove(\"/kaggle/working/LSTM_Fic_model.h5\")\n",
    "# # os.remove(\"/kaggle/working/examples.txt\")\n",
    "# # os.remove(\"/kaggle/working/state.db\")\n",
    "#\n",
    "#\n",
    "# def on_epoch_end(epoch, logs):\n",
    "#     # Function invoked at end of each epoch. Prints generated text.\n",
    "#     examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "#\n",
    "#     # Randomly pick a seed sequence\n",
    "#     seed_index = np.random.randint(len(sentences+testSentences))\n",
    "#     seed = (sentences+testSentences)[seed_index]\n",
    "#\n",
    "#     for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "#         sentence = seed\n",
    "#         examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "#         examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "#         examples_file.write(' '.join(sentence))\n",
    "#\n",
    "#         for i in range(50):\n",
    "#             x_pred = np.zeros((1, seqLen, len(words)))\n",
    "#             for t, word in enumerate(sentence):\n",
    "#                 x_pred[0, t, wordAsKey[word]] = 1.\n",
    "#\n",
    "#             preds = model.predict(x_pred, verbose=0)[0]\n",
    "#             next_index = sample(preds, diversity)\n",
    "#             next_word = intAsKey[next_index]\n",
    "#\n",
    "#             sentence = sentence[1:]\n",
    "#             sentence.append(next_word)\n",
    "#\n",
    "#             examples_file.write(\" \"+next_word)\n",
    "#         examples_file.write('\\n')\n",
    "#     examples_file.write('='*80 + '\\n')\n",
    "#     examples_file.flush()\n",
    "#\n",
    "#\n",
    "# def generator(sentence_list, next_word_list, batch_size):\n",
    "#     index = 0\n",
    "#     while True:\n",
    "#         x = np.zeros((batch_size, seqLen, len(words)), dtype=bool)\n",
    "#         y = np.zeros((batch_size, len(words)), dtype=bool)\n",
    "#         for i in range(batch_size):\n",
    "#             for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "#                 x[i, t, wordAsKey[w]] = 1\n",
    "#             y[i, wordAsKey[next_word_list[index % len(sentence_list)]]] = 1\n",
    "#             index = index + 1\n",
    "#         yield x, y\n",
    "#\n",
    "#\n",
    "# checkPath = \"/kaggle/working/LSTM_Fic_model.h5\"\n",
    "# BATCH_SIZE = 64\n",
    "#\n",
    "# checkpoint = ModelCheckpoint(checkPath, monitor='val_accuracy', save_best_only=True)\n",
    "# print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=2, min_lr=0.0001)\n",
    "# callbacks_list = [checkpoint, print_callback, reduce_lr]\n",
    "#\n",
    "#\n",
    "# examples_file = open(examples, \"a\")\n",
    "#\n",
    "# # comment out below block if picking up training\n",
    "# # model.fit(generator(sentences, nextWordsTrain, BATCH_SIZE),\n",
    "# #                         steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "# #                         epochs=3,\n",
    "# #                         callbacks=callbacks_list,\n",
    "# #                         validation_data=generator(testSentences, testNextWords, BATCH_SIZE),\n",
    "# #                         validation_steps=int(len(testSentences)/BATCH_SIZE) + 1)\n",
    "#\n",
    "#\n",
    "# model_path = \"/kaggle/input/lstm-10e/LSTM_Fic_model.h5\"\n",
    "#\n",
    "# # already have 10 epochs\n",
    "# # load the model after saving\n",
    "# new_model = load_model(model_path)\n",
    "# checkpoint = ModelCheckpoint(checkPath, monitor='val_accuracy', save_best_only=True)\n",
    "# print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.00001)\n",
    "# callbacks_list = [checkpoint, print_callback, reduce_lr]\n",
    "#\n",
    "#\n",
    "# examples_file = open(examples, \"a\")\n",
    "# new_model.fit(generator(sentences, nextWordsTrain, BATCH_SIZE),\n",
    "#                         steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "#                         epochs=2,\n",
    "#                         callbacks=callbacks_list,\n",
    "#                         validation_data=generator(testSentences, testNextWords, BATCH_SIZE),\n",
    "#                         validation_steps=int(len(testSentences)/BATCH_SIZE) + 1)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bhBmmKF2wWX1",
    "outputId": "b395dab6-bbc4-48f1-a676-f74a7d62fbd2",
    "executionInfo": {
     "status": "error",
     "timestamp": 1682565310283,
     "user_tz": 240,
     "elapsed": 513687,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:05:38.276654Z",
     "iopub.status.idle": "2023-04-30T18:05:38.277234Z",
     "shell.execute_reply.started": "2023-04-30T18:05:38.276946Z",
     "shell.execute_reply": "2023-04-30T18:05:38.276981Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ONvFYJHgPP7H",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1682565310286,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": []
  }
 ]
}