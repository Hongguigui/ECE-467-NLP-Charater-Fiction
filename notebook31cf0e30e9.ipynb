{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "import os\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "from keras.utils import pad_sequences\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# simplified chinese tokenizer\n",
    "import jieba\n",
    "import time\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.《》（）+-=()\"\"''/=\"\n",
    "\n",
    "# skipped directories\n",
    "# SKIP = [\"0\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "SKIP = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "# SKIP = []\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "def get_all_items(root: pathlib.Path, exclude):\n",
    "    itemList = []\n",
    "    for item in root.iterdir():\n",
    "        if item.name in exclude:\n",
    "            continue\n",
    "        if item.is_dir():\n",
    "            itemList.append(get_all_items(item, []))\n",
    "            continue\n",
    "        itemList.append(item)\n",
    "    return itemList\n",
    "\n",
    "\n",
    "# begin preprocessing\n",
    "largeDir = pathlib.Path(\"/kaggle/input/wow-mandarin-fictions/\")\n",
    "# largeDir = pathlib.Path(\"./Books\")\n",
    "BookList = get_all_items(largeDir, SKIP)\n",
    "BookList = [item for item in BookList]\n",
    "\n",
    "\n",
    "# clean the dataset\n",
    "# for path in BookList:\n",
    "#     print(path)\n",
    "#     file = open(path, 'r')\n",
    "#     try:\n",
    "#         fileStr = file.read()\n",
    "#     except UnicodeDecodeError as error:\n",
    "#         file.close()\n",
    "#         os.remove(path)\n",
    "#     continue\n",
    "\n",
    "bigString = \"\"\n",
    "\n",
    "for path in BookList:\n",
    "    with open(path, 'r', encoding='gbk') as fiction:\n",
    "        bigString += fiction.read()\n",
    "\n",
    "# methods to strip punctuation and symbols\n",
    "# bigString = re.sub(r\"[%s]+\" %punc, \"\", bigString)\n",
    "bigString = re.sub(r'[^\\w\\s]', '', bigString)\n",
    "\n",
    "# list of the words in their original order\n",
    "allTokens = jieba.lcut(bigString, cut_all=False)\n",
    "t2 = time.time()\n",
    "print(\"Runtime for this cell in seconds: \", t2 - t1)\n",
    "print(\"Corpus length in words: \", len(allTokens))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nFNI0rvYwWXu",
    "outputId": "264c9567-cbe9-4692-b62b-2696e4c56c3c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682558346696,
     "user_tz": 240,
     "elapsed": 62127,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:04:53.414871Z",
     "iopub.execute_input": "2023-04-30T18:04:53.415490Z",
     "iopub.status.idle": "2023-04-30T18:05:38.262779Z",
     "shell.execute_reply.started": "2023-04-30T18:04:53.415457Z",
     "shell.execute_reply": "2023-04-30T18:05:38.260212Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 0.945 seconds.\nPrefix dict has been built successfully.\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_23/3466678853.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[0;31m# list of the words in their original order\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 74\u001B[0;31m \u001B[0mallTokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjieba\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlcut\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbigString\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcut_all\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     75\u001B[0m \u001B[0mt2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Runtime for this cell in seconds: \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt2\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mt1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/jieba/__init__.py\u001B[0m in \u001B[0;36mlcut\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    355\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    356\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mlcut\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 357\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcut\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    358\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    359\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mlcut_for_search\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/jieba/__init__.py\u001B[0m in \u001B[0;36mcut\u001B[0;34m(self, sentence, cut_all, HMM, use_paddle)\u001B[0m\n\u001B[1;32m    323\u001B[0m                 \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mre_han\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mblk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 325\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcut_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mblk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    326\u001B[0m                     \u001B[0;32myield\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    327\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/jieba/__init__.py\u001B[0m in \u001B[0;36m__cut_DAG\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    249\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__cut_DAG\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 250\u001B[0;31m         \u001B[0mDAG\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_DAG\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    251\u001B[0m         \u001B[0mroute\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    252\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcalc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDAG\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mroute\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/jieba/__init__.py\u001B[0m in \u001B[0;36mget_DAG\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m    183\u001B[0m         \u001B[0mN\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    184\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mxrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mN\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 185\u001B[0;31m             \u001B[0mtmplist\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    186\u001B[0m             \u001B[0mi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m             \u001B[0mfrag\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msentence\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "minFreq = 10\n",
    "# got rid of the 4 most common words\n",
    "maxFreq = 1000000\n",
    "wordFreq = {}\n",
    "for token in allTokens:\n",
    "    wordFreq[token] = wordFreq.get(token, 0) + 1\n",
    "\n",
    "# le -> 223736 de -> 578590\n",
    "skipWords = set()\n",
    "for k, v in wordFreq.items():\n",
    "    if wordFreq[k] < minFreq or wordFreq[k] > maxFreq:    \n",
    "        skipWords.add(k)        \n",
    "\n",
    "words = set(allTokens)\n",
    "print(\"Unique words before filter: \", len(words))\n",
    "print(\"To reduce vocab size, neglect words with appearances < \", minFreq)\n",
    "print(\"To reduce vocab size, neglect words with appearances > \", maxFreq)\n",
    "words = sorted(set(words) - skipWords)\n",
    "print(\"Unique words after filter: \", len(words))\n",
    "\n",
    "wordAsKey = dict((c, i) for i, c in enumerate(words))\n",
    "intAsKey = dict((i, c) for i, c in enumerate(words))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yAEhwscIwWXy",
    "outputId": "e0da0173-33be-40c7-d3a5-0c12be1c3b13",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682558348613,
     "user_tz": 240,
     "elapsed": 1941,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:05:38.263927Z",
     "iopub.status.idle": "2023-04-30T18:05:38.265005Z",
     "shell.execute_reply.started": "2023-04-30T18:05:38.264680Z",
     "shell.execute_reply": "2023-04-30T18:05:38.264725Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "t1 = time.time()\n",
    "seqLen = 50\n",
    "step = 1\n",
    "sequences = []\n",
    "nextWords = []\n",
    "seqIgnored = 0\n",
    "for i in range(0, len(allTokens) - seqLen, step):\n",
    "    if len(set(allTokens[i:i+seqLen+1]).intersection(skipWords)) == 0:\n",
    "        sequences.append(allTokens[i:i + seqLen])\n",
    "        nextWords.append(allTokens[i + seqLen])\n",
    "    else:\n",
    "        seqIgnored += 1\n",
    "\n",
    "print(\"Number of sequences ignored: \", seqIgnored)\n",
    "print(\"Number of remaining sequences: \", len(sequences))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Runtime for this cell in seconds: \", t2 - t1)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "pggs1zNAwWXz",
    "outputId": "98cd9378-6f1a-4dc4-bddb-f4cac02c6b2a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682558405241,
     "user_tz": 240,
     "elapsed": 56630,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:05:38.267558Z",
     "iopub.status.idle": "2023-04-30T18:05:38.268117Z",
     "shell.execute_reply.started": "2023-04-30T18:05:38.267825Z",
     "shell.execute_reply": "2023-04-30T18:05:38.267853Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "(sentences, nextWords), (testSentences, testNextWords) = shuffle_and_split_training_set(sequences, nextWords)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dABfcglLwWX0",
    "outputId": "a8468807-9fa2-4e39-a86f-764050b3750e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682558407689,
     "user_tz": 240,
     "elapsed": 2481,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:05:38.270483Z",
     "iopub.status.idle": "2023-04-30T18:05:38.271082Z",
     "shell.execute_reply.started": "2023-04-30T18:05:38.270767Z",
     "shell.execute_reply": "2023-04-30T18:05:38.270794Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model\n",
    "def get_model(dropout=0.2):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128), input_shape=(seqLen, len(words))))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "QdOxPqChwWX1",
    "outputId": "4224a27f-fe66-4800-b06d-686694993531",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682558408650,
     "user_tz": 240,
     "elapsed": 965,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:05:38.273654Z",
     "iopub.status.idle": "2023-04-30T18:05:38.274257Z",
     "shell.execute_reply.started": "2023-04-30T18:05:38.273958Z",
     "shell.execute_reply": "2023-04-30T18:05:38.273986Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "import os, psutil\n",
    "\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "process = psutil.Process()\n",
    "print(process.memory_info().rss/1024/1024)\n",
    "print(psutil.virtual_memory())\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "examples = \"/kaggle/working/examples.txt\"\n",
    "\n",
    "# os.remove(\"/kaggle/working/LSTM_Fic_model.h5\")\n",
    "# os.remove(\"/kaggle/working/examples.txt\")\n",
    "# os.remove(\"/kaggle/working/state.db\")\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+testSentences))\n",
    "    seed = (sentences+testSentences)[seed_index]\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, seqLen, len(words)))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t, wordAsKey[word]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = intAsKey[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "            examples_file.write(\" \"+next_word)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "    examples_file.flush()\n",
    "\n",
    "\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, seqLen, len(words)), dtype=bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t, wordAsKey[w]] = 1\n",
    "            y[i, wordAsKey[next_word_list[index % len(sentence_list)]]] = 1\n",
    "            index = index + 1\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "checkPath = \"/kaggle/working/LSTM_Fic_model.h5\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkPath, monitor='val_accuracy', save_best_only=True)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=2, min_lr=0.0001)\n",
    "callbacks_list = [checkpoint, print_callback, reduce_lr]\n",
    "\n",
    "\n",
    "examples_file = open(examples, \"a\")\n",
    "\n",
    "# comment out below block if picking up training\n",
    "# model.fit(generator(sentences, nextWords, BATCH_SIZE),\n",
    "#                         steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "#                         epochs=3,\n",
    "#                         callbacks=callbacks_list,\n",
    "#                         validation_data=generator(testSentences, testNextWords, BATCH_SIZE),\n",
    "#                         validation_steps=int(len(testSentences)/BATCH_SIZE) + 1)\n",
    "\n",
    "\n",
    "model_path = \"/kaggle/input/lstm-10e/LSTM_Fic_model.h5\"\n",
    "\n",
    "# already have 10 epochs\n",
    "# load the model after saving\n",
    "new_model = load_model(model_path)\n",
    "checkpoint = ModelCheckpoint(checkPath, monitor='val_accuracy', save_best_only=True)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, print_callback, reduce_lr]\n",
    "\n",
    "\n",
    "examples_file = open(examples, \"a\")\n",
    "new_model.fit(generator(sentences, nextWords, BATCH_SIZE),\n",
    "                        steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                        epochs=2,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=generator(testSentences, testNextWords, BATCH_SIZE),\n",
    "                        validation_steps=int(len(testSentences)/BATCH_SIZE) + 1)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "bhBmmKF2wWX1",
    "outputId": "b395dab6-bbc4-48f1-a676-f74a7d62fbd2",
    "executionInfo": {
     "status": "error",
     "timestamp": 1682565310283,
     "user_tz": 240,
     "elapsed": 513687,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "execution": {
     "iopub.status.busy": "2023-04-30T18:05:38.276654Z",
     "iopub.status.idle": "2023-04-30T18:05:38.277234Z",
     "shell.execute_reply.started": "2023-04-30T18:05:38.276946Z",
     "shell.execute_reply": "2023-04-30T18:05:38.276981Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ONvFYJHgPP7H",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1682565310286,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "Hongyu Wu",
      "userId": "17314858635426623037"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}